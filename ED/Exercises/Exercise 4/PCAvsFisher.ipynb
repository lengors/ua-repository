{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Input space models to reduce dimension\n",
    "\n",
    "PCA versus Discriminant Fisher (LDA) models : \n",
    "\n",
    "1. PCA model is estimated without using the information of the labels\n",
    "2. Fisher  discriminant is estimated taking into account the labels of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numeric\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# graphics\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.gridspec as gridspec \n",
    "#\n",
    "#Added version check for recent scikit-learn 0.18 checks\n",
    "from distutils.version import LooseVersion as Version\n",
    "from sklearn import __version__ as sklearn_version\n",
    "###########\n",
    "from numpy import linalg as LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The data set\n",
    "\n",
    "X=np.array([[1,2],[2,3],[3, 3],[4,5],[5,5],[1,0],[2,1],[3,1],[3,2],[5,3],[6,5]])\n",
    "\n",
    "print(X.shape)\n",
    "y=np.array([0,0,0,0,0,1,1,1,1,1,1])\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.scatter(X[y==0, 0], X[y==0, 1], color='red', marker='^', alpha=0.8)\n",
    "plt.scatter(X[y == 1, 0], X[y == 1, 1], color='blue', marker='o', alpha=0.8)\n",
    "plt.grid()\n",
    "plt.xlim([-1, 7])\n",
    "plt.ylim([-1, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "print(pca)\n",
    "print('eigenvectors \\n', pca.components_)\n",
    "print('singular values ', pca.singular_values_)\n",
    "print('normalized cumulative sum of eigenvalues \\n', pca.explained_variance_ratio_)\n",
    "print(' mean vector ', pca.mean_)\n",
    "\n",
    "print('Projections of class 0 \\n ', X_pca[y==0])\n",
    "print('Projections of class 1 \\n ', X_pca[y==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=pca.mean_\n",
    "#print(a[0], a[1])\n",
    "plt.scatter(X[y==0, 0], X[y==0, 1], color='red', marker='^', alpha=0.8)\n",
    "plt.scatter(X[y == 1, 0], X[y == 1, 1], color='blue', marker='o', alpha=0.8)\n",
    "# The center of the axis\n",
    "plt.scatter(a[0],a[1], color='black', marker='*', alpha=1)\n",
    "plt.grid()\n",
    "plt.xlim([-1, 7])\n",
    "plt.ylim([-1, 6])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "The data was pojected into the direction related with largest singular value.\n",
    "\n",
    "1. The values of the projecions into the direction of the largest singular value allow to discriminate the two classes? \n",
    "2. The point \"*\" marks the mean vector.  What is the direction of the eigenvector \n",
    "3. Singular value and eigenvalue are related. How?\n",
    "4. Note that the input to the decomposition is the non-centered data. You have a new point $\\mathbf{z}=(4,4)^T$ to project into main the direction of the PCA model. The procedure is the following\n",
    " -- project the  point after subtracting the mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Repeating to \n",
    "pca = PCA(n_components=1)\n",
    "X_pca = pca.fit_transform(X)\n",
    "## Reconstruction\n",
    "X1=pca.inverse_transform(X_pca)\n",
    "X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The reconstruction Error\n",
    "\n",
    "a=X-X1\n",
    "print('The squared error', LA.norm(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Representing in the 2-D Space\n",
    "\n",
    "plt.scatter(X1[y==0, 0], X1[y==0, 1], color='red', marker='^', alpha=0.5)\n",
    "plt.scatter(X1[y == 1, 0], X1[y == 1, 1], color='blue', marker='o', alpha=0.5)\n",
    "plt.scatter(a[0],a[1], color='black', marker='*', alpha=1)\n",
    "plt.grid()\n",
    "plt.xlim([-1, 7])\n",
    "plt.ylim([-1, 6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "1-  The data was reconstructed to the original space.  Compare the value of the squared error with the value of the discarded singular value.\n",
    "2. The call to the model was initiated with __pca = PCA(n_components= )___, e.g, having as input the number of components to to estimated. What are the alternatives?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fisher Discriminant  (LDA) \n",
    "The data is organized into $C$ classes\n",
    "\n",
    "The main steps of the Fisher:\n",
    "\n",
    "1. Compute the means: the global mean $\\mathbf{m}$, the mean for each class $\\mathbf{m}_c, c=1, \\ldots C$\n",
    "2. Compute scatter matrices\n",
    "    - the scatter matrix within classes\n",
    "$$\n",
    "\\mathbf{S}_w= \\sum \\limits_{c=1}^C \\mathbf{S}_c\n",
    "$$\n",
    "where $\\mathbf{S}_c$ is the scatter matrix computed with the data belonging to $c$- class\n",
    "     - the scatter matrix between classes is defined using the means\n",
    "$$\n",
    "\\mathbf{S}_B= \\sum \\limits_{c=1}^C N_c \\mathbf{S}_{m_c}\n",
    "$$\n",
    "where $N_c$ is the number of elements in classe $c$ and \n",
    "$$\n",
    "\\mathbf{S}_{m_c}=  (\\mathbf{m}_c- \\mathbf{m}) (\\mathbf{m}_c- \\mathbf{m}) ^T \n",
    "$$\n",
    "3.  Perform the eigendecomposition of $\\mathbf{S}_w^{-1} \\mathbf{S}_B $ and project the data into $C-1$ eigenvectors related with the $C-1$ largest eigenvalues.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "1. Write the code for the toy data set\n",
    "2.  In this case the LDA model has only one eigenvector.  Project the data and comment the results \n",
    "3. The point $\\mathbf{z}=(4,4)^T$ belongs to the class 0 or to the class 1?\n",
    "3. Find in scikit-learn one LDA implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### To illustrate your results . The variable \"projections\" should have the result of your code.\n",
    "#inx=(y==0)\n",
    "#inx=inx.ravel()\n",
    "#L=sum(inx)\n",
    "#plt.scatter(projections[inx], np.zeros((L, 1)),marker='^',color='r')  \n",
    "#inx=(y==1)\n",
    "#inx=inx.ravel()\n",
    "#L=sum(inx)\n",
    "#plt.scatter(projections[inx], np.zeros((L, 1)),marker='o',color='blue')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In scikit-learn\n",
    "#if Version(sklearn_version) < '0.18':\n",
    "#    from sklearn.lda import LDA\n",
    "#else:\n",
    "#    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "#lda = LDA(store_covariance=True)\n",
    "#lda.fit(X, y)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
